{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文書を渡すとjanomeで処理してリストで返す関数\n",
    "from janome.tokenizer import Tokenizer\n",
    "from janome.analyzer import Analyzer\n",
    "from janome.charfilter import *\n",
    "from janome.tokenfilter import *\n",
    "import pathlib\n",
    "import pandas as pd \n",
    "\n",
    "def before_filter(before_st):\n",
    "    #Unicode正規化・半角記号除去・全角記号除去・＜＞タグ除去・改行除去\n",
    "    char_filters = [UnicodeNormalizeCharFilter('NFKC'),\n",
    "                    RegexReplaceCharFilter('[!/:;%#\\@,_✨✅♪\\'\\$&\\?\\(\\)~\\.=\\+\\-…]+', ''),\n",
    "                    RegexReplaceCharFilter('[『！”＃＄％＆’（）＝～｜‘｛＋＊｝＜＞？＿－＾￥＠「；：」、。・]', ''),\n",
    "                    RegexReplaceCharFilter('<.*?>', ''), \n",
    "                    RegexReplaceCharFilter('[\\n|\\r|\\t]', '')\n",
    "                   ]\n",
    "    #名詞だけ取得・アルファベット小文字化・基本形\n",
    "    token_filters = [NumReplaceFilter(),\n",
    "                     OneCharReplaceFilter(),\n",
    "                     POSKeepFilter(['名詞']),\n",
    "                     LowerCaseFilter(),\n",
    "                     ExtractAttributeFilter('base_form')]\n",
    "    #設定からフィルターを作る\n",
    "    a = Analyzer(char_filters=char_filters, token_filters=token_filters)\n",
    "    #文章をanalyzerに掛け処理する\n",
    "    after_st = a.analyze(before_st)\n",
    "    \n",
    "    #ストップワードを除去する\n",
    "    stop_words = []\n",
    "    path = 'Japanese.txt'\n",
    "    with open(path, encoding=\"utf-8_sig\") as f:\n",
    "        stop_words = f.read().split('\\n')\n",
    "\n",
    "    after_st = [x for x in after_st if x not in stop_words]\n",
    "\n",
    "    return after_st\n",
    "\n",
    "#名詞内の数字をゼロに置き換える\n",
    "class NumReplaceFilter(TokenFilter):\n",
    "\n",
    "    def apply(self, tokens):\n",
    "        for token in tokens:\n",
    "            parts = token.part_of_speech.split(',')\n",
    "            if (parts[0] == '名詞' and parts[1] == '数'):\n",
    "                token.surface = '0'\n",
    "                token.base_form = '0'\n",
    "                token.reading = 'ゼロ'\n",
    "                token.phonetic = 'ゼロ'\n",
    "            yield token\n",
    "\n",
    "class OneCharReplaceFilter(TokenFilter):\n",
    "\n",
    "    def apply(self, tokens):\n",
    "        for token in tokens:\n",
    "            # 一文字の単語を除去\n",
    "            if re.match('^[あ-んア-ンa-zA-Z0-9ー]$', token.surface):\n",
    "                continue\n",
    "\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "def wordCount(folder, file):\n",
    "    #CSVを読み込む\n",
    "    path = \"\" #ファイルを直下に置いてない場合はパスを入れる\n",
    "    #filetest = 'paypaytest.csv'\n",
    "    filetest = folder + file\n",
    "    \n",
    "    #pandasでCSVファイルを開き、3列目のツイート内容だけdfに取得する\n",
    "    df = pd.read_csv(filetest, usecols=[2], encoding='utf8', engine='python')\n",
    "    \n",
    "    \n",
    "    #単語のカウント表を作る\n",
    "    #dicScore = {} \n",
    "    dfCount = pd.DataFrame(columns=['count'])\n",
    "    \n",
    "    #ツイート本文を形態素解析し、名詞・動詞・形容詞だけ抽出したリストを作成する\n",
    "    num = 0\n",
    "    for row in df['ツイート']:\n",
    "        word_list = before_filter(row)\n",
    "        for word in word_list:\n",
    "            #リストを頻出単度をカウントする辞書に追加する、辞書になければ追加する\n",
    "            if not any(dfCount.index == word): \n",
    "                dfCount.loc[word] = 1\n",
    "            else:\n",
    "                dfCount.at[word, 'count'] += 1\n",
    "    \n",
    "    #得点表を降順ソート\n",
    "    count_sorted = dfCount.sort_values('count', ascending=False)\n",
    "    \n",
    "    #結果を保存\n",
    "    count_sorted.to_csv(folder+ 'Count_' + file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'paypay/Count_paypay_2020-06-13.csv'\n",
    "df = pd.read_csv(name, usecols=[2], encoding='utf8', engine='python')\n",
    "count_sorted = dfCount.sort_values('count', ascending=False)\n",
    "count_sorted.to_csv(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount('paypay/', 'paypay_2020-06-11.csv')\n",
    "wordCount('paypay/', 'paypay_2020-06-12.csv')\n",
    "wordCount('paypay/', 'paypay_2020-06-13.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
